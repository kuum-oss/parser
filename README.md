# ğŸ“š Fantasy World Scraper & Analyzer

> **A high-performance asynchronous web scraper built with Node.js, coupled with a strict Java domain analyzer.**

## ğŸ“– Overview

This project is a dual-stack solution designed to harvest and analyze fantasy literature data. It automates the collection of thousands of book records from *fantasy-worlds.org* and provides a Java-based architecture for processing and filtering that data.

The core of this project is the **Intelligent Parser**, designed to handle thousands of pages concurrently without triggering server bans.

---

## ğŸ•·ï¸ The Parser (Node.js)

**Location:** `parser/Ğ¿Ğ°Ñ€ÑĞµÑ€.js`

The scraper is engineered for speed and reliability. It crawls pages 1 through 2070 to extract book metadata including titles, authors, ratings, and vote counts.



### Key Features

* **ğŸš€ High Concurrency** â€“ Uses `Promise.all` to process multiple pages simultaneously.
* **â±ï¸ Hz Batching Strategy** â€“ Implements a `CONCURRENCY` limit of 5 and a `BATCH_DELAY` of 800ms to respect server rate limits.
* **ğŸ“Š Visual Feedback** â€“ Integrated `cli-progress` bar to track real-time scraping status.
* **ğŸ’ª Robust Extraction** â€“ Utilizes `cheerio` (jQuery for Node) to parse complex DOM structures and clean data on the fly.

### Code Snippet: Batch Processing

```javascript
// Smart batching to prevent IP bans
for (let p = START_PAGE; p <= END_PAGE; p += CONCURRENCY) {
    const batch = [];
    for (let i = 0; i < CONCURRENCY && p + i <= END_PAGE; i++) {
        batch.push(parsePage(p + i));
    }
    await Promise.all(batch);
    await sleep(BATCH_DELAY); // Politeness delay
}
```

---

## â˜• The Processor (Java)

**Location:** `src/main/java`

Once the data is collected, the Java application handles the business logic, enrichment, and filtering.

* **ğŸ“‚ CSV Ingestion** â€“ Custom `CsvBookReader` to parse raw text files into `Book` domain objects.
* **ğŸ” Stream Filtering** â€“ Uses Java Streams to filter books by minimum rating thresholds.
* **ğŸŒ API Simulation** â€“ The `BookEnrichmentService` mocks external calls to Google Books to "enrich" local data with fresh ratings.

---

## ğŸ› ï¸ Tech Stack

| Component   | Technology     | Description                     |
| ----------- | -------------- | ------------------------------- |
| **Scraper** | **Node.js**    | Runtime environment             |
|             | `axios`        | HTTP Client for fetching HTML   |
|             | `cheerio`      | DOM parsing and data extraction |
|             | `cli-progress` | Terminal progress bar           |
| **Backend** | **Java 25**    | Core application language       |
|             | Maven          | Dependency management           |
|             | Stream API     | Functional data processing      |

---

## ğŸš€ Usage

### 1ï¸âƒ£ Run the Parser

Collect the data first.

```bash
cd parser
npm install axios cheerio cli-progress
node Ğ¿Ğ°Ñ€ÑĞµÑ€.js
```

*Output: Generates `fantasy_worlds_books.csv`*

### 2ï¸âƒ£ Run the Java App

Process the data.

```bash
mvn clean install
mvn exec:java -Dexec.mainClass="Main"
```

*Note: Ensure the CSV file path in `Main.java` matches your generated file.*

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ ğŸ•·ï¸ parser/
â”‚   â”œâ”€â”€ ğŸ§  Ğ¿Ğ°Ñ€ÑĞµÑ€.js                # Main scraping logic
â”‚   â”œâ”€â”€ ğŸ’¾ fantasy_worlds_books.csv # Scraped data
â”‚   â””â”€â”€ ğŸ¨ assets/
â”‚       â””â”€â”€ ğŸ–¼ scraper-demo.gif       # Optional GIF preview
â”œâ”€â”€ â˜• src/main/java/
â”‚   â”œâ”€â”€ ğŸ“‘ csv/                     # Data Ingestion
â”‚   â”œâ”€â”€ ğŸ“š domain/                  # POJOs (Book.java)
â”‚   â”œâ”€â”€ âš™ï¸ filter/                  # Business Rules
â”‚   â”œâ”€â”€ ğŸŒ google/                  # API Clients
â”‚   â””â”€â”€ ğŸš€ Main.java                # Entry Point
â””â”€â”€ ğŸ§© pom.xml                      # Maven Config
```


